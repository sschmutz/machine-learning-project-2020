---
title: "Dataset and Objectives - Machine Learning Project 2020"
subtitle: "Harry Chirayil, Christopher Keim, Stefan Schmutz"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(ggrepel)
library(ggtext)
library(tidytext)
library(scales)
library(knitr)
library(here)

update_geom_defaults("text", list(family = "Source Sans Pro", size = 6))
update_geom_defaults("label", list(family = "Source Sans Pro", size = 6))
update_geom_defaults("label_repel", list(family = "Source Sans Pro", size = 6))
```

```{r read_datasets, message=FALSE, warning=FALSE}

headlines_20min <-
  read_csv(here("data", "headlines_20min.csv")) %>%
  mutate(source = "20min")

headlines_nzz <-
  read_csv(here("data", "headlines_nzz.csv")) %>%
  mutate(source = "NZZ")

# source: https://github.com/solariz/german_stopwords/blob/master/german_stopwords_plain.txt
stop_words_german <-
  read_csv(here("data", "stop_words_german.txt"), col_names = c("word"))

# Combine the two datasets
headlines <-
  bind_rows(headlines_20min, headlines_nzz) %>%
  mutate(source = factor(source, levels = c("NZZ", "20min")),
         object = factor(str_to_title(object), levels = c("Title", "Teaser"))) 

# remove replicates and split text in order to have one word per line
headlines_tidy <-
  headlines %>%
  distinct(object, text, source, .keep_all = TRUE) %>%
  unnest_tokens(word, text)

```


## Dataset
The dataset includes **news headlines** (title and short teaser) from two online news sites [20min.ch](https://www.20min.ch/) and [nzz.ch](https://www.nzz.ch/).


### Format
\begin{tabular}{ll}
date\_time & date and time of web scraping in Coordinated Universal Time (UTC) \\  
object & type of text, either "title" or "teaser" \\   
order & order headline appeared on website, makes it possible to link title and teaser \\  
text & full text, all in lowercase and punctuation marks removed \\  
\end{tabular}

&nbsp;

Following are the first few rows of the `headlines_nzz.csv` raw data:

```{r dataset-structure}
headlines_nzz %>%
  select(-source) %>%
  print(n = 5)
```


### Data collection
All titles and teasers (if available) from the titlepage were collected twice a day (6am and 6pm local time) between 2019-02-04 and 2020-01-21.  
While the amount of unique titles are comparable between the two sources, there were more teasers available from the 20min titlepage (see Table \ref{tab:dataset-statistics}).

```{r dataset-statistics, message=FALSE}
n_unique <-
  headlines %>%
  group_by(source, object) %>%
  summarise(n_unique = length(unique(text))) %>%
  arrange(object, source)

n_unique %>%
  knitr::kable(caption = "\\label{tab:dataset-statistics}Sample size per class",
               format.args = list(big.mark = "'", scientific = FALSE))
```


### Objective
Can we predict the source of a news headline based on the chosen words?  
We might want to try with or without stop words removal, different vocabulary sizes and compare the performances using k-fold cross-validation.


### Data visualisation
To gain some insights, basic data visualization was done on the dataset where replicates (stories listed multiple times) were only kept once.  
Figure \ref{fig:headline-length} shows the headline length (words) distributions within the different categories. It can be seen that nzz headlines are typically a bit longer and with a larger spread compared to 20min headlines.  
Figure \ref{fig:word-frequencies} compares the word frequencies from the two sources. Stop words, words that are very common and typically not very useful for classification were removed (source: [github.com/solariz/german_stopwords](https://github.com/solariz/german_stopwords/blob/master/german_stopwords_plain.txt)).  
It's visible that many of the most frequently used words occur at different rates in headlines of the two news-sites. This could hint that it's possible to classify headlines based on the words used.

```{r headline-length, fig.cap="\\label{fig:headline-length}Length distribution of headlines in words. Dotted lines represent means per group.", out.width = "90%", fig.align = "center", message=FALSE, warning=FALSE}

headlines_word_length <-
  headlines_tidy %>%
  group_by(date_time, object, order, source) %>%
  summarise(total_words = n()) %>%
  ungroup()

headlines_word_length_mean <-
  headlines_word_length %>%
  group_by(object, source) %>%
  summarise(mean_words = mean(total_words)) %>%
  ungroup()

n_unique <-
  n_unique %>%
  mutate(x_pos = if_else(object == "Title", 25, 50))



headline_length_dist <-
  headlines_word_length %>%
  ggplot(aes(total_words, fill = source)) +
  geom_histogram(aes(y=..density..), bins = 35, alpha = 0.9) +
  geom_vline(data = headlines_word_length_mean, aes(xintercept = mean_words), color = "grey70", linetype = "dashed", size = 0.5) +
  facet_grid(source ~ object, scales = "free_x") +
  scale_fill_manual(values = c("#A40E4C", "#0d2880")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0, 0.3, by = 0.1)) +
  geom_text(y = 0.22, aes(label = paste0("n = ", format(n_unique, big.mark = "'")), x = x_pos), data = n_unique, color = "grey60", size = 5) +
  theme_minimal(base_family = "Source Sans Pro",
                base_size = 16) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank(),
        plot.title.position = "plot",
        plot.title = element_markdown(),
        plot.subtitle = element_markdown()) +
  labs(x = "Number of words",
       y = NULL, 
       title = "**Headline length distribution**",
       subtitle = "amount of words in <span style='color:#A40E4C'>**NZZ**</span> and <span style='color:#0d2880'>**20min**</span> headlines")

#ggsave(here("figures", "headline_length_dist.png"), headline_length_dist, width = 10, height = 6)

headline_length_dist
```



```{r word-frequencies, fig.cap="\\label{fig:frequent-words}Word frequencies in title and teaser per source (not including stop words)", out.width = "90%", fig.align = "center", warning=FALSE}

word_frequencies <-
  headlines_tidy %>%
  anti_join(stop_words_german, by = "word") %>%
  count(source, word) %>%
  group_by(source) %>% # alternatively also by object
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  pivot_wider(names_from = source, values_from = proportion, values_fill = 0) %>%
  mutate(proportion_differences = NZZ-`20min`,
         big_proportion_differences_class = if_else(abs(proportion_differences) >= 0.0009, TRUE, FALSE),
         proportion_differences_class = case_when(
           proportion_differences <= -0.0009 ~ "frequent in 20min",
           proportion_differences >= 0.0009 ~ "frequent in NZZ",
           TRUE ~ "no larg differences"
         ))

pos <- position_jitter(width = 0.03, height = 0.03, seed = 1)

word_frequencies_plot <-
  word_frequencies %>%
  filter(`20min` != 0,
         NZZ != 0) %>%
  ggplot(aes(x = `20min`, y = NZZ, color = proportion_differences_class)) +
  geom_abline(color = "gray40", lty = 2) +
  geom_point(alpha = 0.3, size = 2.5, position = pos) +
  geom_label_repel(data = filter(word_frequencies, big_proportion_differences_class == TRUE), aes(label = word, fill = proportion_differences_class), color = "white", segment.color = "gray50", alpha = 0.9, segment.alpha = 0.5, position = pos) +
  scale_x_log10(labels = percent_format(accuracy = 0.001), limits = c(0.00001, 0.01)) +
  scale_y_log10(labels = percent_format(accuracy = 0.001), limits = c(0.00001, 0.01)) +
  scale_color_manual(values = c("#0d2880", "#A40E4C", "gray83")) +
  scale_fill_manual(values = c("#0d2880", "#A40E4C")) +
  theme_minimal(base_family = "Source Sans Pro",
                base_size = 16) +
  theme(legend.position = "none",
        plot.title.position = "plot",
        plot.title = element_markdown(),
        plot.subtitle = element_markdown()) +
  labs(title = "**Proportion differences**",
       subtitle = "reveal words which are more prevalent in <span style='color:#A40E4C'>**NZZ**</span> or <span style='color:#0d2880'>**20min**</span> headlines")

#ggsave(here("figures", "word_frequencies.png"), word_frequencies_plot, width = 10, height = 6)

word_frequencies_plot
```


### Heads up
Some headlines might be advertisement. We could detect those by looking at how often they were visible. We'd expect advertisement to appear more frequently compared to actual news headlines.  

For the 20min dataset we have a teaser for almost every title, this is not the case for the nzz dataset where very often we only have a title without teaser.


## Chosen Estimator
Following the flow diagram below (see Figure \ref{fig:ml-map}), we might want to try Naive Bayes or Linear Support Vector Classification.  

```{r ml-map, fig.cap="Choosing the right ML estimator, source: scikit-learn.org", echo=FALSE, out.width="400px", fig.align="center"}
knitr::include_graphics(here("figures", "ml_map.png"))
```
